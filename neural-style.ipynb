{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luozegithub/pytorch-beginner/blob/master/neural-style.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "T6ZsECh5n9fV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import PIL.Image as Image\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "img_size = 512\n",
        "loader = transforms.Compose([\n",
        "        transforms.Resize([ img_size,img_size]), \n",
        "        transforms.ToTensor()\n",
        "#     ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "]) \n",
        "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def load_img(img_path):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img = loader(img).unsqueeze(0)\n",
        "    return img.to(device, torch.float)\n",
        "\n",
        "\n",
        "\n",
        "def show_img(tensor,title=None):\n",
        "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
        "  \n",
        "    image = image.squeeze(0) # torch.Size([3, 512, 512]) 去掉之前unsqueeze添加的维度\n",
        "    image = unloader(image)\n",
        "  \n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(1) # pause a bit so that plots are updated\n",
        "    plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SbaGymiMoNUF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class Content_Loss(nn.Module):\n",
        "    def __init__(self, target, weight):\n",
        "        super(Content_Loss, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "\n",
        "        self.target = target.detach() * self.weight\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = self.criterion(input * self.weight, self.target)\n",
        "        out = input.clone()\n",
        "        return out\n",
        "# retain_graph就是用来保存计算反向时的graph的，当retain_graph为true时，那么就可以多次单独backward，而不怕上一次的梯度消失！\n",
        "    def backward(self, retain_variabels=True):\n",
        "        self.loss.backward(retain_graph=True)\n",
        "        return self.loss\n",
        "\n",
        "\n",
        "      \n",
        "# 为了计算styleloss 我们设计了Gram矩阵\n",
        "'''\n",
        "Gram矩阵是在这个特征图上定义的。每个特征图的大小一般是 MxNxC 或者是 CxMxN，\n",
        "\n",
        "Gram矩阵是如何定义的呢？首先Gram矩阵的大小是有特征图的channel 决定的，等于 CxC，那么每一个Gram矩阵的元素，也就是 Gram(i, j) 等于多少呢？先把特征图中第 i 层和第 j 层取出来，\n",
        "这样就得到了两个 MxN的矩阵，然后将这两个矩阵对应元素相乘然后求和就得到了 Gram(i, j)，同理 Gram 的所有元素都可以通过这个方式得到。这样 Gram 中每个元素都可以表示两层特征图的一种组合，就可以定义为它的风格。\n",
        "实现的时候我们直接使用view将其变化成(c，（h*w）),然后乘以转置矩阵得到最终结果\n",
        "\n",
        "然后风格的差异就是两幅图的 Gram 矩阵的差异，就像内容的差异的计算方法一样，计算一下这两个矩阵的差就可以量化风格的差异。\n",
        "'''\n",
        "class Gram(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Gram, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        a, b, c, d = input.size()\n",
        "        feature = input.view(a * b, c * d)\n",
        "        gram = torch.mm(feature, feature.t())\n",
        "        gram /= (a * b * c * d)\n",
        "        return gram\n",
        "\n",
        "\n",
        "class Style_Loss(nn.Module):\n",
        "    def __init__(self, target, weight):\n",
        "        super(Style_Loss, self).__init__()\n",
        "        self.weight = weight\n",
        "        # 因为这里的target是用style图传入网络中得到的。它是一个Variable！有着自己的计算图。detach()的作用就是将这个结点“截断”，使得其变成叶子节点\n",
        "        # 截断反向传播的梯度流\n",
        "        self.target = target.detach() * self.weight\n",
        "        self.gram = Gram()\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def forward(self, input):\n",
        "        G = self.gram(input) * self.weight\n",
        "        self.loss = self.criterion(G, self.target)\n",
        "        out = input.clone()\n",
        "        return out\n",
        "\n",
        "    def backward(self, retain_variabels=True):\n",
        "        self.loss.backward(retain_graph=True)\n",
        "        return self.loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KERjCg8oaPN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "10212172-e531-4cd4-d0d6-0a68fab54cd1"
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "vgg = vgg.cuda()\n",
        "\n",
        "content_layers_default = ['conv_4']\n",
        "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "\n",
        "def get_style_model_and_loss(style_img,\n",
        "                             content_img,\n",
        "                             cnn=vgg,\n",
        "                             style_weight=1000,\n",
        "                             content_weight=1,\n",
        "                             content_layers=content_layers_default,\n",
        "                             style_layers=style_layers_default):\n",
        "\n",
        "    content_loss_list = []\n",
        "    style_loss_list = []\n",
        "\n",
        "    model = nn.Sequential()\n",
        "\n",
        "    model = model.cuda()\n",
        "    gram = Gram()\n",
        "    gram = gram.cuda()\n",
        "\n",
        "    i = 1\n",
        "    for layer in cnn:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            name = 'conv_' + str(i)\n",
        "            model.add_module(name, layer)\n",
        "\n",
        "            if name in content_layers_default:\n",
        "                target = model(content_img)\n",
        "                content_loss = Content_Loss(target, content_weight)\n",
        "                model.add_module('content_loss_' + str(i), content_loss)\n",
        "                content_loss_list.append(content_loss)\n",
        "\n",
        "            if name in style_layers_default:\n",
        "                target = model(style_img)\n",
        "                target = gram(target)\n",
        "                style_loss = Style_Loss(target, style_weight)\n",
        "                model.add_module('style_loss_' + str(i), style_loss)\n",
        "                style_loss_list.append(style_loss)\n",
        "\n",
        "            i += 1\n",
        "        if isinstance(layer, nn.MaxPool2d):\n",
        "            name = 'pool_' + str(i)\n",
        "            model.add_module(name, layer)\n",
        "\n",
        "        if isinstance(layer, nn.ReLU):\n",
        "            name = 'relu' + str(i)\n",
        "            model.add_module(name, layer)\n",
        "\n",
        "# now we trim off the layers after the last content and style losses\n",
        "#     for i in range(len(model) - 1, -1, -1):\n",
        "#         if isinstance(model[i], Content_Loss) or isinstance(model[i], Style_Loss):\n",
        "#             break\n",
        "#     model = model[:(i + 1)]\n",
        "\n",
        "# 将整个模型，以及损失层的list全部返回\n",
        "    return model, style_loss_list, content_loss_list\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.torch/models/vgg19-dcbb9e9d.pth\n",
            "574673361it [00:07, 77794962.56it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "koT7NVyAodOT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "cae15a75-3196-4860-f4a0-b7281a81c400"
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_input_param_optimier(input_img):\n",
        "    \"\"\"\n",
        "    input_img is a Variable\n",
        "    \"\"\"\n",
        "    input_param = nn.Parameter(input_img.data)\n",
        "    optimizer = optim.LBFGS([input_param])\n",
        "    return input_param, optimizer\n",
        "\n",
        "\n",
        "def run_style_transfer(content_img, style_img, input_img, num_epoches=300):\n",
        "    print('Building the style transfer model..')\n",
        "    model, style_loss_list, content_loss_list = get_style_model_and_loss(\n",
        "        style_img, content_img)\n",
        "    input_param, optimizer = get_input_param_optimier(input_img)\n",
        "\n",
        "    print('Opimizing...')\n",
        "    epoch = [0]\n",
        "    while epoch[0] <= num_epoches:\n",
        "\n",
        "        def closure():\n",
        "            input_param.data.clamp_(0, 1)\n",
        "\n",
        "            model(input_param)\n",
        "            style_score = 0\n",
        "            content_score = 0\n",
        "            # 首先梯度置0\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "\n",
        "'''\n",
        "\n",
        "其实这个“backward”只是一个普通的函数！并不是重载内部的backward！ \n",
        "在这个“backward”中主要是调用criterion的backward，然后返回这个loss，\n",
        "好让外面能拿到相应的损失。 \n",
        "\n",
        "'''\n",
        "            for sl in style_loss_list:\n",
        "                style_score += sl.backward()\n",
        "            # content层与style层甚至可以分开来backward！\n",
        "            for cl in content_loss_list:\n",
        "                content_score += cl.backward()\n",
        "\n",
        "            epoch[0] += 1\n",
        "            if epoch[0] % 50 == 0:\n",
        "                print('run {}'.format(epoch))\n",
        "                print('Style Loss: {:.4f} Content Loss: {:.4f}'.format(\n",
        "                    style_score.data.item(), content_score.data.item()))\n",
        "                print()\n",
        "\n",
        "            return style_score + content_score\n",
        "\n",
        "        optimizer.step(closure)\n",
        "        \n",
        "        # a last correction...\n",
        "        input_param.data.clamp_(0, 1)\n",
        "\n",
        "    return input_param.data\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-a31105fa9f1f>\"\u001b[0;36m, line \u001b[0;32m43\u001b[0m\n\u001b[0;31m    for sl in style_loss_list:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "gtLO4yOxo-uP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "style_img = load_img('./style.png')\n",
        "\n",
        "content_img = load_img('./content.png')\n",
        "\n",
        "input_img = content_img.clone()\n",
        "out = run_style_transfer(content_img, style_img, input_img, num_epoches=300)\n",
        "\n",
        "\n",
        "# Style Loss: 0.3079 Content Loss: 5.1049"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CiuX-ZuK70iA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_img(out，title='Output Image')\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9extvvSTuvr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "\n",
        "img_path = './content.png'\n",
        "\n",
        "# 使用cv2 读入的格式是b g r，而plt 和PIL默认是r g b\n",
        "img = cv2.imread(img_path)\n",
        "b,g,r = cv2.split(img)\n",
        "img = cv2.merge([r,g,b])\n",
        "plt.subplot(1,4,1)\n",
        "plt.title('cv2')\n",
        "plt.imshow(img)\n",
        " \n",
        "#plt\n",
        "img = plt.imread(img_path)\n",
        "plt.subplot(1,4,2)\n",
        "plt.title('plt')\n",
        "plt.imshow(img)\n",
        " \n",
        "#PIL\n",
        "img = Image.open(img_path)\n",
        "plt.subplot(1,4,3)\n",
        "plt.title('PIL')\n",
        "plt.imshow(img)\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "transforms.ToTensor(),\n",
        "# from tensor to <class 'PIL.Image.Image'>\n",
        "transforms.ToPILImage(),\n",
        "transforms.RandomCrop((300,300)),\n",
        "])\n",
        "\n",
        "plt.subplot(1,4,4)\n",
        "img = Image.open(img_path).convert('RGB')\n",
        " \n",
        "img3 = transform_test(img)\n",
        "plt.imshow(img3)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}